{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/n1coleabt/CPE124-CW2/blob/main/pa2_solution(draft).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upy_H2JTdwyf"
      },
      "source": [
        "# CS 124 Programming Assignment 3: Logistic Regression (`Winter 2025`)\n",
        "\n",
        "In the last assignment, you have used a `Naive Bayes` classifier to classify\n",
        "disaster aid messages.\n",
        "In this assignment, you are going to perform the same task using a\n",
        "`Logistic Regression` (`LR`) classifier.\n",
        "We will evaluate your model using the same metrics as the previous assignments.\n",
        "Because we are sharing the data and the task between the two assignments,\n",
        "many sections of this notebook will be the same as those in our previous\n",
        "assignment.\n",
        "\n",
        "Let's remember our task:\n",
        "Victims of natural disasters have urgent needs for food, water, shelter, medicine, and other forms of aid.\n",
        "These needs are often communicated through text messages, social media posts, and local newspapers. Because of their\n",
        "ability to automatically process large amounts of text, `NLP` techniques can play an important role in ensuring that people receive potentially life-saving aid.\n",
        "Our goal will be to perform text classification on messages sent in the aftermath of natural disasters.\n",
        "\n",
        "We will be utilizing a `Python` module called `NumPy` in this assignment,\n",
        "similar to the last one.\n",
        "If you feel like you need a refresher on `NumPy`, you can always revisit the\n",
        "`NumPy` tutorial (`numpy_tutorial.ipynb`) we shared along with the previous\n",
        "assignment.\n",
        "\n",
        "**You are encouraged to work with a partner!** We want the assignments in `CS 124` to bring you joy.\n",
        "One way to ensure this is to work with a partner!\n",
        "You are free to work with one other partner in our assignments.\n",
        "If you choose to work with a partner, we ask that each partner work on each part of the assignment in jointly instead of splitting parts.\n",
        "The partnership decision is independent for each assignment, so you can choose to work alone, work with the same partner or work with a different partner in the future assignments, which is a good way to meet your fellow classmates!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0T5kYg_3Y7S"
      },
      "source": [
        "<a id=\"contents\"></a>\n",
        "## Contents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DXv8PA2-hU5"
      },
      "source": [
        "Listed below are the contents of the assignment. In the `Data Exploration`\n",
        "section, you will look into the Disaster Aid Classification (`Triage`) dataset\n",
        "we will use in the assignment.\n",
        "In the `Logistic Regression` section, you will implement a `Logistic Regression`\n",
        "classifier to determine whether a message sent in the aftermath of a natural\n",
        "disaster is about aid.\n",
        "In the `Tips` section, we share some useful tips for your implementation.\n",
        "In the `Evaluation on the Triage Dataset` section, you will evaluate your\n",
        "`Logistic Regression` classifier on the `Triage` dataset.\n",
        "Please read through all of this notebook before you start working through the assignment.\n",
        "In the last section, we will breifly touch on the `hyperparameters` you can adjust for your `Logistic Regression` classifier.\n",
        "Note that the links may not work on `Google Colab`.\n",
        "\n",
        "* [`Part 1. Data Exploration`](#data_exploration)\n",
        "* [`Part 2. Logistic Regression`](#logistic_regression)\n",
        "* [`Part 3. Evaluation on the Triage Dataset`](#evaluation_triage)\n",
        "* [`Part 4. Note on Hyperparameters`](#hyperparameters)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pBXnsqTqHu6"
      },
      "source": [
        "<a id=\"roadmap\"></a>\n",
        "## Roadmap\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwzw4SXt-tJy"
      },
      "source": [
        "As an overview, there are `5` methods you need to implement in this assignment:\n",
        "* In `Part 2.1 Sigmoid`: **`sigmoid()`** function\n",
        "* In `Part 2.2 Logistic Loss`: **`logistic_loss()`** function\n",
        "* In `Part 2.3 Gradient Descent`: **`gradient_descent()`** function\n",
        "* In `Part 2.4 Logistic Regression Classifier`: **`__init__()`**, **`train()`**, **`classify()`**, and **`get_weights()`** methods of the **[`LogisticRegressionClassifier(Classifier)`](#logistic_regression)** class\n",
        "* In `Part 3.2 Sanity Check`: **`reflection_response()`** function, which is a short answer question\n",
        "\n",
        "Here is how your implementation will be evaluated:\n",
        "* In `Part 3. Evaluation on the Triage Dataset`, your implementation will be evaluated with respect to the `triage` dataset.\n",
        "  * In `Part 3.1 Accuracy`, you will check the accuracy of your `Logistic Regression` model both on the `train` and `dev` sets, each with and without stop words filtered via the vectorizer's parameter. Hence, there will be a total of `4` accuracy tests in this section. We recommend going back to your implementation if the accuracies you get in this section are far from what we have provided.\n",
        "    In addition to what you will see in this section, our autograder will run `2` more tests on the same dataset, this time using a `test` set, with or without stop words. **Note: Unlike PA2, you will not need to write any code to filter stop words.**\n",
        "    To see the autograder tests, you can submit your notebook on `Gradescope`.\n",
        "  * In `Part 3.2 Sanity Check`, we will check the answer you include in the `reflection_response()` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4gZzwRnPpWw"
      },
      "source": [
        "<a id=\"submitting\"></a>\n",
        "## Submitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuKzBe8IYqBT"
      },
      "source": [
        "**Submit your empty assignment to Gradescope now to see the autograder output!**\n",
        "You will submit your assignment via [`Gradescope`](www.gradescope.com), where we have an autograder set up.\n",
        "You can submit your assignment any number of times before the deadline.\n",
        "As a general rule of thumb, we recommend submitting early and often in any `Computer Science` class if you have the option, to prevent any last minute errors with autograders.\n",
        "Submitting early also helps gauge how you are doing on the visible test cases of the autograder and gives you a chance to fix your submission accordingly.\n",
        "In fact, start with submitting your assignment now (even if you haven't coded anything), so that you are familiar with the submission process and know what kind of autograder feedback is available to you.\n",
        "You can re-submit as you make progress.\n",
        "Don't forget to update your submission with your final version once you are done!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y15qB5bxwR3N"
      },
      "source": [
        "**Partners.**\n",
        "You are welcome (and encouraged) to work with one partner.\n",
        "If you do work with a partner, only one of you needs to submit the assignment on `Gradescope` and tag the other as a group member."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCMk0AEJYupX"
      },
      "source": [
        "**Environment.**\n",
        "Before you submit, make sure your code works in the environment described in the [`Environment Check`](#environment_check) section, as this is the environment our autograder will be run on.\n",
        "If you have completed the setup steps in `PA0` and run this notebook in the `cs124` environment you created according to the instructions, you are good!\n",
        "Note that you must not use any other dependencies (such as other `Python` modules), as doing so may cause the autograder to fail!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DjXAbXQYxcD"
      },
      "source": [
        "**Saving Your Notebook**.\n",
        "Make sure to save the recent changes in your notebook before you submit.\n",
        "This is especially important if you are running your notebook on `Google Colab` as connection quality sometimes cause your notebook to be in an unsaved state.\n",
        "The following error is also common on `Google Colab`, if the file you are working on is open in more than one tabs, so we are recommending keeping copies of your work if you are collaborating with your partner on `Colab`.\n",
        "```\n",
        "This file was updated remotely or in another tab. To force a save, overwriting the last update, select Save from the File menu\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMSPTRReQFXo"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**Files.**\n",
        "Once you are done, you only need to submit the file listed below.\n",
        "**DO NOT** alter the file name.\n",
        "```\n",
        "pa3.ipynb\n",
        "```\n",
        "\n",
        "**Custom Dependencies.**\n",
        "Sometimes you may want to put parts of your code into `.py` files and call them from your notebook instead of having all your functions in the notebook, or utilize extra datasets.\n",
        "If this is the case, please put your extra files in a folder\n",
        "named `deps/` (this folder should be on the same level as `pa3.ipynb`)\n",
        "and upload a `zip` file (any name is fine) containing this folder and\n",
        "`pa3.ipynb` to submit on `Gradescope`.\n",
        "Note that these should be at the top directory of the `.zip` file (e.g. they should not be in a directory in the `.zip` file, as this will lead our autograder to fail at finding them).\n",
        "To prevent this, ensure that you are only zipping the items mentioned, and not the folder containing them.\n",
        "`Gradescope` will then automatically `unzip` the folder so that your\n",
        "submission contains the following.\n",
        "```\n",
        "deps/\n",
        "pa3.ipynb\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRZdYG4raTMj"
      },
      "source": [
        "**Submission Script.**\n",
        "For your convenience, we are providing the following submission script that lets you automatically create a `zip` file to submit.\n",
        "Simply run it and submit `submission.zip` to `Gradescope`.\n",
        "Note that the script assumes that you have the `zip` utility installed.\n",
        "You would need to install it if you don't already have it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UmC4xynbYbc",
        "outputId": "86c58c37-e203-4ac3-9120-96869a59b1f9",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "if [[ ! -f \"./pa3.ipynb\" ]]\n",
        "then\n",
        "    echo \"WARNING: Did not find notebook in Jupyter working directory. This probably means you're running on Google Colab. You'll need to go to File->Download .ipynb to download your notebok and other files, then zip them locally. See the README for more information.\"\n",
        "else\n",
        "    echo \"Found notebook file, creating submission zip...\"\n",
        "    zip -r submission.zip pa3.ipynb deps/\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC7Ji3nxbo2Z"
      },
      "source": [
        "If you are running your notebook on `Google Colab`, see the `README` for instructions on how to submit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHHWQZvAZPZ4"
      },
      "source": [
        "**Autograder.**\n",
        "Once you submit, double check the autograder output to ensure that your submission didn't cause any error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnnkT9eMhBlJ"
      },
      "source": [
        "<a id=\"environment_check\"></a>\n",
        "## Environment Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cyv1CAE-o-m"
      },
      "source": [
        "This assignment assumes that you have correctly set up the `cs124` conda environment and installed the required `Python` modules.\n",
        "The cell below checks that you are running the correct version of `Python` and activated the `cs124` conda environment.\n",
        "If you are running the notebook on `Google Colab`, you need to download the `Python` extra modules we use in the assignment separately.\n",
        "If you get an error running this cell, it means that you are either using the wrong `Conda` environment\n",
        "or Python version!\n",
        "If the latter, please exit this notebook, kill the notebook server with `CTRL-C`, and\n",
        "try running:\n",
        "\n",
        "`$ conda activate cs124`\n",
        "\n",
        "Then restarting your notebook server with\n",
        "\n",
        "`$ jupyter notebook`\n",
        "\n",
        "If this doesn't work, you should go back and follow the installation instructions in `PA0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "rUVLxfnHiPbK",
        "tags": [
          "essential"
        ]
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import sys\n",
        "assert sys.version_info.major == 3 and sys.version_info.minor >= 8  # Allow newer versions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdAYv1X28NIT"
      },
      "source": [
        "<a id=\"setup\"></a>\n",
        "## Part 0. Setup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ8pMJZJDIJu"
      },
      "source": [
        "**Getting the Necessary Files.** The cell below downloads the necessary files we will use in this assignment, if you don't already have them.\n",
        "This may be the case, for example, if you are running the assignment on `Google Colab`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "LkwF0Sb28d7-",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "if [[ ! -d \"./data\" ]]\n",
        "then\n",
        "    echo \"Missing extra files. Downloading...\"\n",
        "    git clone https://github.com/cs124/pa3-logistic-regression.git\n",
        "    cp -r ./pa3-logistic-regression/{data,deps,util.py} .\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dSKvfgi-Mna"
      },
      "source": [
        "**Importing Modules.** Run the next cell to import the necessary modules we will use in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "OhuxbSxv-99D",
        "tags": [
          "essential"
        ]
      },
      "outputs": [],
      "source": [
        "\"\"\" Modules included in the Python Standard Library \"\"\"\n",
        "\n",
        "# collections module contain useful Python data structures, such as dictionaries\n",
        "# with special properties\n",
        "from collections import defaultdict\n",
        "\n",
        "# operator module allows us to use functions such as add() instead of operators\n",
        "# such as +\n",
        "import operator\n",
        "\n",
        "# random modules allows us to insert randomization to our code\n",
        "import random\n",
        "\n",
        "# typing module contains type objects. We will use these types to ensure that\n",
        "# the inputs and outputs passed to the functions you will be implementing are\n",
        "# of the correct type\n",
        "from typing import List, Dict, Union"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "SXB-ukJxVXMj",
        "tags": [
          "essential"
        ]
      },
      "outputs": [],
      "source": [
        "\"\"\" Third party modules \"\"\"\n",
        "\n",
        "# numpy is a widely used scientific computing package, allowing us to do large\n",
        "# matrix operations efficiently\n",
        "import numpy as np\n",
        "\n",
        "# matplotlib is a popular library used by researchers to plot graphs\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sklearn is a popular machine learning library, providing useful tools for\n",
        "# machine learning tasks\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "t5qPbMwQVC_a",
        "tags": [
          "essential"
        ]
      },
      "outputs": [],
      "source": [
        "\"\"\" Our custom functions and classes \"\"\"\n",
        "\n",
        "# Helper functions and classes we will use later\n",
        "from util import load_data, Classifier, Example, evaluate, remove_stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma5Fcxt9_X1d"
      },
      "source": [
        "**WARNING:** **DO NOT** import or use any other packages except the ones imported above and other packages in the Python standard library.\n",
        "This means you should not use `spaCy`, `NLTK`, `gensim`, or other functionality in `scikit-learn` besides `CountVectorizer`, even though those are provided in the `conda` environment we set up for you.\n",
        "If your solution uses any such extra dependencies it will fail the autograder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L98XWjQyjUw8"
      },
      "source": [
        "<a id=\"data_exploration\"></a>\n",
        "## Part 1. Data Exploration for the Triage Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8srZPWSq5IQ"
      },
      "source": [
        "As usual, the first thing to do is to understand and characterize the data!\n",
        "The data for this assignment contains about `26K` documents from several major natural disasters, as listed below.\n",
        "\n",
        "* [Earthquake in Haiti (2010)](https://en.wikipedia.org/wiki/2010_Haiti_earthquake)\n",
        "* [Floods in Pakistan (2010)](https://en.wikipedia.org/wiki/2010_Pakistan_floods)\n",
        "* [Earthquake in Chile (2010)](https://en.wikipedia.org/wiki/2010_Chile_earthquake)\n",
        "* [Hurricane Sandy in North America (2012)](https://en.wikipedia.org/wiki/Hurricane_Sandy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u-uRvlRPhDP"
      },
      "source": [
        "**Dataset.** The documents in our dataset are either text messages, social media (`Twitter`) posts, or snippets from news articles.\n",
        "In addition to the specific events listed above the dataset contains a number of news articles spanning dozens of different disasters.\n",
        "All messages have been translated and annotated by humans on the crowdsourcing platform `CrowdFlower` (now branded under [`Appen`](https://appen.com/)).\n",
        "However, some of the translations are not perfect, and you may encounter some words in other\n",
        "languages.\n",
        "Unfortunately, `NLP` researchers often have to work with `messy` data.\n",
        "If you are curious about the crowdsourcing translation effort for messages\n",
        "from Haiti in particular, feel free to check out [this paper](https://nlp.stanford.edu/pubs/munro2010translation.pdf).\n",
        "\n",
        "Your task is to classify each document as being aid-related, class `aid`, or not aid-related, class `not`.\n",
        "Messages that are aid-related include individuals' requests for food, water, or shelter etc.\n",
        "The `aid` class also includes news reports about dire situations and disaster relief efforts.\n",
        "Below are several examples of aid-related documents, belonging to class `aid`.\n",
        "```\n",
        "Hello Good Morning We live on 31 Delmas we are without water without food and what we had have finished Please do something for us!\n",
        "```\n",
        "```\n",
        "I am sending this SMS from Layah district for my sister whose house has got destroyed in a flood\n",
        "So, the problem she faces now is that she hasn't got any 'Watan Card'or any financial aid from the government.\n",
        "She has 5 children too.\n",
        "```\n",
        "```\n",
        "Redcross came to my house and gave my family food ...\n",
        "Guess were not getting power anytime soon . #sandy #RedCross\n",
        "```\n",
        "```\n",
        "Relief officials have stressed the vital importance of bringing in clean drinking water and sanitation equipment to avoid deadly epidemics that in a\n",
        "worst case scenario could claim as many or more lives than the tsunami itself.\n",
        "```\n",
        "Below are several examples of non-aid-related documents, belonging to class `not`:\n",
        "```\n",
        "A cold front is found over Cuba this morning.\n",
        "It could cross Haiti tomorrow.\n",
        "Isolated rain showers are expected over our region tonight.\n",
        "```\n",
        "```\n",
        "Hurricane : A storm which New Yorkers use as an excuse to drink and eat junk\n",
        "food in their pajamas for 48 hours . #sandy\n",
        "```\n",
        "```\n",
        "By secret ballot, the Council elected Pakistan, Bahrain and the Republic of\n",
        "Korea from the Asian States, while Iran and Saudi Arabia did not receive enough\n",
        "votes to qualify.\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCm95WnCcw-p"
      },
      "source": [
        "**Training, Validation, and Test Sets.**\n",
        "The data is divided into a `training` set, `development` (`validation`) set, and `test` set.\n",
        "Recall that the `training` set is used to learn, compute the statistics\n",
        "for, your model.\n",
        "These statistics are then used to classify the documents in the\n",
        "`development` and `test` sets.\n",
        "For this assignment, you have access to the\n",
        "`training` set and the `dev` set.\n",
        "The test `set` is hidden, but your submission will be evaluated on it as well.\n",
        "Although we do not share the specific test examples we use, the autograder we provide will output target accuracies your classifier should achieve for each of these sets for full credit.\n",
        "All you need to do to see these is to submit your assignment.\n",
        "Remember that you can always re-submit!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2YYfAJYb5Bm"
      },
      "source": [
        "**Exploration.**\n",
        "Let's take a look at some of the data.\n",
        "We have defined a `Dataset` class for you to store the loaded data in a way we can easily access later, and a function `load_data()` to load it in the format we want.\n",
        "You do not need to check the specifics of our `Dataset` class, we will explain exactly how you will use it in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4SSGY25eIoS",
        "outputId": "f99e304f-b31d-4655-d22b-541ab8131998",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'util.Dataset'>\n"
          ]
        }
      ],
      "source": [
        "# Load our dataset\n",
        "dataset = load_data(\"./data/triage\")\n",
        "\n",
        "# Check that the type of our dataset is the Dataset class we defined for you\n",
        "# in util.py.\n",
        "print(type(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJDNkOtsgV66"
      },
      "source": [
        "We are interested in the following two fields of the `Dataset` class: `train` and `dev`.\n",
        "Given that `dataset` is an instance of the `Dataset` class, we can access these fields with `dataset.train` and `dataset.dev`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUgCeJsUgkRK",
        "outputId": "c853feb6-5c3d-4d42-af36-a8bfa28078b2",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset.train contains 21046 examples\n",
            "<class 'util.Example'>\n"
          ]
        }
      ],
      "source": [
        "print(f\"dataset.train contains {len(dataset.train)} examples\")\n",
        "print(type(dataset.train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ut61mksEmR1r"
      },
      "source": [
        "Each of `dataset.train` and `dataset.dev` is a list of `Example`'s.\n",
        "Similar to `Dataset`, `Example` is a class we have defined to represent each data point we have.\n",
        "The `Example` class has two fields we will be concerned with: `words` and `label`.\n",
        "`words` field corresponds to the list of words making up the example.\n",
        "`label` field corresponds to the label of the data point, which is an integer that can only take one of two values: `1` for `aid` and `0` for `not` aid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwRvqrY5h6lz",
        "outputId": "15c47be6-fc59-4b20-a3b4-562e614bfc54",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First training example:\n",
            "Words: ['house', 'distroying', 'but', 'we', 'must', 'that', 'many', 'people', \"who's\", 'living', 'in', 'port', 'au', 'prince', 'goes', 'to', 'countryside', 'and', 'at', 'this', 'time', 'they', 'find', 'no', 'help', 'so', 'my', 'advive', 'is', 'to', 'delivered', 'held', 'to', 'the', 'people', 'outside', 'the', 'capital']\n",
            "Label: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"First training example:\")\n",
        "print(\"Words: {}\".format(dataset.train[0].words))\n",
        "print(\"Label: {}\".format(dataset.train[0].label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lRaJN9dhorh"
      },
      "source": [
        "In summary, we use the custom defined classes `Example` and `Dataset` to represent our dataset and data points in a nice format so that we can work with them easily.\n",
        "This is achieved by the `load_data()` function we called earlier.\n",
        "At a high level, when we pass it the path `./data/triage`, `load_data()` finds the `CSV` files located there.\n",
        "Within each of these `CSV` files, each line is a single example, consisting\n",
        "of a document (`string`) and a corresponding label.\n",
        "The function `load_data()` reads each line in the `CSV` files as a new `Example`.\n",
        "It tokenizes each document it reads and sets the `words` field of the `Example` class to be a list of words.\n",
        "You can check the `CSV` files located in `./data/triage` to see the original format of these files.\n",
        "\n",
        "Note that the data you are given is already preprocessed; all punctuation has been removed, except hashtags and apostrophes, and all text has been converted to lowercase.\n",
        "If you were working on a new task, you would likely need to complete the preprocessing step yourself.\n",
        "Depending on the specific `NLP` task, preprocessing can significantly improve performance.\n",
        "You do not need to do any additional preprocessing for our task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94Qmp95Mc2d9"
      },
      "source": [
        "<a id=\"logistic_regression\"></a>\n",
        "## Part 2. Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nfmLc4hpvCs"
      },
      "source": [
        "Now that we have our data set up, we can get started on implementing our\n",
        "`Logistic Regression` classifier!\n",
        "First, let's start off with some preliminaries to double-check our `NumPy` skills\n",
        "and our understanding of the `Logistic Regression` algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iihGlk-c7fRQ"
      },
      "source": [
        "<a id=\"sigmoid\"></a>\n",
        "### Part 2.1 Sigmoid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAf_mTHJ7fRR"
      },
      "source": [
        "One of the building blocks of `Logistic Regression` is the `Sigmoid` function,\n",
        "which we described in lecture.\n",
        "It is the method we use to convert the outputs of our computation\n",
        "$(z = w * x + b)$ from a real number between negative infinity and infinity to\n",
        "a probability between `0` and `1`.\n",
        "Your first task is to implement the `Sigmoid` function below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "tags": [
          "todo"
        ],
        "id": "zVa4ERSk7fRR"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    TODO: Implement the sigmoid function.\n",
        "\n",
        "    Args:\n",
        "        x: A numpy array.\n",
        "    Returns:\n",
        "        s: The numpy array with sigmoid applied element-wise\n",
        "\n",
        "    HINTS:\n",
        "    * Use np.exp() because your input can be a numpy array\n",
        "    \"\"\"\n",
        "    # CODE START\n",
        "    s = 1 / (1 + np.exp(-x))\n",
        "    return s\n",
        "    # CODE END"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([-2, -1, 0, 1, 2])\n",
        "print(sigmoid(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tCuhr6P-GS6",
        "outputId": "b5fbbc6c-2aba-4c3c-83d4-39c087353eff"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.11920292 0.26894142 0.5        0.73105858 0.88079708]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TtELfDk7fRS"
      },
      "source": [
        "Once you are done with your implementation, try visualizing it to\n",
        "double-check that we have the right idea.\n",
        "Hopefully the graph looks reasonable!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Create a numpy array containing evenly separated numbers, from -10 to 10 (inclusive)\n",
        "x = np.arange(-10, 10.01, 0.01)  # Adjusted to include 10\n",
        "\n",
        "# Call our sigmoid function on the newly created array\n",
        "y = sigmoid(x)\n",
        "\n",
        "# Plot\n",
        "plt.plot(x, y, color='blue', lw=2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"sigmoid(x)\")\n",
        "plt.title(\"Sigmoid Function\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "XeS8IXpn9Un_",
        "outputId": "77e03826-0a4f-4c1b-f2a7-0670e064c06b"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBA52Hk27fRU"
      },
      "source": [
        "<a id=\"logistic_loss\"></a>\n",
        "### Part 2.2 Logistic Loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0mpXzqF7fRU"
      },
      "source": [
        "Now there's just one more building block we need: the `logistic loss` function,\n",
        "which also known as the `cross-entropy loss`.\n",
        "Intuitively, the loss function is simply a way to measure how far our prediction\n",
        "$y_{\\text{pred}} = \\sigma(WX + b)$ is from the true label $y_{\\text{true}}$.\n",
        "Using the formulation covered in lecture and in the notes, implement the\n",
        "logistic loss function in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "tags": [
          "todo"
        ],
        "id": "_9ejXZ0m7fRV"
      },
      "outputs": [],
      "source": [
        "def logistic_loss(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    TODO: Implement the computation of the logistic loss function.\n",
        "\n",
        "    Your function should take in not just a single predicted label\n",
        "    and true label, but a vector of predictions and a vector of true labels.\n",
        "    This shouldn't affect your implementation much, as numpy allows you to\n",
        "    operate naturally on vectors.\n",
        "\n",
        "    Your function should return the average logistic loss over all the\n",
        "    examples as a single float.\n",
        "\n",
        "    Args:\n",
        "        y_pred: A 1D vector of predicted labels for each example, of shape\n",
        "        (num_examples,).\n",
        "        y_true: A 1D vector of true labels for each example, of shape\n",
        "        (num_examples,).\n",
        "\n",
        "    Returns:\n",
        "        float: The average logistic loss over all examples as a float.\n",
        "\n",
        "    HINTS:\n",
        "    * np.mean() and np.log() could be helpful!\n",
        "    * If you run into \"RuntimeWarning: divide by zero encountered in log\"\n",
        "        issues, try adding a very small number epsilon (like 1e-8) to the\n",
        "        input any timE you call np.log(). This will ensure that the input to\n",
        "        the log is never exactly 0, which gives an undefined result.\n",
        "    \"\"\"\n",
        "    # CODE START\n",
        "    # Small epsilon value to prevent log(0) errors\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # Clip y_pred values to be within the range (epsilon, 1 - epsilon)\n",
        "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "\n",
        "    # Compute logistic loss\n",
        "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "\n",
        "    return loss\n",
        "    # CODE END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pEUsO8F7fRW"
      },
      "source": [
        "To check our implementation, let's compare it to our intuition about how the\n",
        "loss should behave.\n",
        "The loss is supposed to represent how far our prediction is from the true label.\n",
        "In other words, if our prediction is way off, the loss should be very high.\n",
        "If our prediction gets closer to the true value, the loss should drop towards\n",
        "`0`.\n",
        "Let's consider a few cases to understand how the loss should change on specific\n",
        "input!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([1, 0, 1, 0])\n",
        "y_pred = np.array([0.9, 0.1, 0.8, 0.2])\n",
        "\n",
        "loss = logistic_loss(y_pred, y_true)\n",
        "print(\"Logistic Loss:\", loss)  # Expected output: A small positive float value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3BteykP_qLV",
        "outputId": "1e5ceb39-f52f-4a7f-c744-5429935d8e40"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Loss: 0.164252033486018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "tags": [
          "exploration"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdoKoqrg7fRW",
        "outputId": "b73ee160-5e01-4d94-cbd5-e1456fdac723"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted = 0.0, True = 1 : Loss = 18.420680743952367\n",
            "Predicted = 0.1, True = 1 : Loss = 2.3025850929940455\n",
            "Predicted = 0.3, True = 1 : Loss = 1.2039728043259361\n",
            "Predicted = 0.5, True = 1 : Loss = 0.6931471805599453\n",
            "Predicted = 0.7, True = 1 : Loss = 0.35667494393873245\n",
            "Predicted = 0.9, True = 1 : Loss = 0.10536051565782628\n",
            "Predicted = 0.99, True = 1 : Loss = 0.01005033585350145\n",
            "Predicted = 0.999999, True = 1 : Loss = 1.000000500029089e-06\n",
            "Predicted = 1, True = 1 : Loss = 1.0000000100247594e-08\n"
          ]
        }
      ],
      "source": [
        "def print_loss(y_pred, y_true):\n",
        "    print(\"Predicted = {}, True = {} : Loss = {}\".format(\n",
        "          y_pred, y_true, logistic_loss(np.array([y_pred]),\n",
        "                                        np.array([y_true]))))\n",
        "\n",
        "print_loss(0.0, 1)\n",
        "print_loss(0.1, 1)\n",
        "print_loss(0.3, 1)\n",
        "print_loss(0.5, 1)\n",
        "print_loss(0.7, 1)\n",
        "print_loss(0.9, 1)\n",
        "print_loss(0.99, 1)\n",
        "print_loss(0.999999, 1)\n",
        "print_loss(1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdv7dSFs7fRX"
      },
      "source": [
        "Take a look at the results.\n",
        "What happens when $y_{pred} = y_{true}$? What happens when $y_{pred} = 0$ and\n",
        "$y_{true} = 1$?\n",
        "Do they agree with what you expect from the formula for the logistic loss?\n",
        "Why might this behavior be okay in practice, given how we are going to use the\n",
        "`logistic loss` (i.e. `gradient descent`)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mantfBH7fRX"
      },
      "source": [
        "<a id=\"gradient_descent\"></a>\n",
        "### Part 2.3 Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOYSHZN87fRX"
      },
      "source": [
        "Now that you have implemented the `sigmoid()` and `logistic_loss()`\n",
        "functions, our next step is to introduce our optimization function.\n",
        "We will use the `gradient descent` algorithm to learn the parameters for our\n",
        "`logistic regression` classifier from the data.\n",
        "We have implemented the majority of this algorithm for you, but you will need to compute:\n",
        "\n",
        "- The gradient of the loss with respect to `W` (`dW`).\n",
        "- Compute the gradient of the loss with respect to `b` (`db`).\n",
        "- Update `W`\n",
        "- Update `b`\n",
        "\n",
        "The lecture slides give the derivative of the loss for individual elements of the vector in terms of $w_j$ and $b$. However, in your python implementation, you'll want to compute the gradient updates using the matrix form.\n",
        "\n",
        "To compute `dW`, `db` (note that $m$ is the batch size):\n",
        "\n",
        "- $dW = \\frac{1}{m} X_{\\text{batch}}^T \\left(A - Y_{\\text{batch}}\\right)$\n",
        "- $db = \\frac{1}{m} \\sum_{i=1}^{m} \\Bigl(A^{(i)} - Y_{\\text{batch}}^{(i)}\\Bigr)$\n",
        "\n",
        "To update $W$ and $b$, you want to utilize the gradients of the loss function to move $W$ and $b$ in the opposite direction, while taking into account the learning rate.\n",
        "\n",
        "**To implement these formulas in Python, you may find the following numpy functions useful:**\n",
        "- `np.expand_dims(A - Y_batch, axis=1)` can reshape `(A - Y_batch)` from shape `(m,)` to `(m, 1)` if needed for matrix multiplication.  \n",
        "  - When computing `dW`, you will need to multiply `(A - Y_batch)` by each row in `X_batch`:\n",
        "  - If `(A - Y_batch)` is shape `(m,)`, you can do elementwise multiplication with `X_batch` by temporarily reshaping it to `(m, 1)` using `np.expand_dims(A - Y_batch, axis=1)`.\n",
        "  - Then taking the mean along `axis=0` gives you a vector of shape `(num_features,)`.\n",
        "- `np.mean(...)` computes the mean across a specified axis\n",
        "\n",
        "\n",
        "We will have a deeper discussion on `gradient descent` in the context of neural\n",
        "networks.\n",
        "While you only have to write a few lines of code for this section, we encourage you to take a look and try to\n",
        "understand what the function is doing as a whole, as well as what each of the\n",
        "parameters (`alpha`, `epsilon`, `num_iterations`) does.\n",
        "You can also see how it makes use of the `sigmoid()` and `logistic_loss()`\n",
        "functions we just implemented!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "MfWXy7PkqK0S",
        "tags": [
          "essential"
        ]
      },
      "outputs": [],
      "source": [
        "# DO NOT CHANGE\n",
        "def gradient_descent(X: np.ndarray,\n",
        "                     Y: np.ndarray,\n",
        "                     batch_size: int = 2000,\n",
        "                     alpha: float = 0.5,\n",
        "                     num_iterations: int = 1000,\n",
        "                     print_every: int = 100,\n",
        "                     epsilon: float = 1e-8) -> (np.ndarray, float):\n",
        "    \"\"\"\n",
        "    Runs batch gradient descent on the provided data and returns the resulting\n",
        "    trained weight vector and bias.\n",
        "\n",
        "    Args:\n",
        "        X: A numpy array of shape (num_examples, num_features) containing\n",
        "           the training data.\n",
        "        Y: A numpy array of shape (num_examples,) containing the training\n",
        "            labels.\n",
        "        batch_size: The number of examples in each batch.\n",
        "        alpha: The learning rate for gradient descent.\n",
        "        num_iterations: The number of iterations to run gradient descent\n",
        "                        for.\n",
        "        print_every: How often (after how many iterations) to print the\n",
        "                    loss and iteration number.\n",
        "        epsilon: The early stopping condition. When the absolute change\n",
        "                 in the loss is less than epsilon, gradient descent will\n",
        "                 stop early.\n",
        "\n",
        "    TODO:\n",
        "        Fill in the code to compute dW, db, W, and b using the gradient descent equations\n",
        "        for logistic regression explained above.\n",
        "\n",
        "        Once you have the gradients of the loss function, use them to update W and b to move\n",
        "        in the opposite direction, taking into account the learning rate.\n",
        "\n",
        "        - dW: the gradient of the loss with respect to W\n",
        "        - db: the gradient of the loss with respect to b\n",
        "        - W: the weight vector\n",
        "        - b: the bias vector\n",
        "\n",
        "    Returns:\n",
        "        (np.ndarray, float): The learned weight vector W and bias b\n",
        "    \"\"\"\n",
        "    # DO NOT CHANGE\n",
        "    # Initialize the weight vector W to all zeros of shape (num_features, )\n",
        "    W = np.zeros((X.shape[1],))\n",
        "    b = 0\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    # Initialize loss, which is updated every iteration\n",
        "    loss = 0\n",
        "    for i in range(num_iterations):\n",
        "        # If the batch size is greater than or equal to the entire dataset,\n",
        "        # we just use all the data in one batch\n",
        "        if batch_size >= X.shape[0]:\n",
        "            X_batch = X\n",
        "            Y_batch = Y\n",
        "        else:\n",
        "            # Otherwise, we randomly sample examples of size 'batch_size' to form the current batch\n",
        "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
        "            batch_indices = np.random.randint(X.shape[0], size=batch_size)\n",
        "            X_batch = X[batch_indices, :]\n",
        "            Y_batch = Y[batch_indices]\n",
        "\n",
        "        # 1) Compute predicted probabilities for our batch using the sigmoid function\n",
        "        # A has shape (batch_size, )\n",
        "        A = sigmoid(np.dot(X_batch, W) + b)\n",
        "\n",
        "        # TODO: 2) Compute the gradients dW and db,\n",
        "        # and update the params W and b using the gradients and learning rate alpha.\n",
        "        # dW has shape (num_features, )\n",
        "        # db is a scalar\n",
        "\n",
        "        # CODE START\n",
        "        # 2) Compute gradients\n",
        "        m = batch_size  # Number of samples in the batch\n",
        "        dW = (1 / m) * np.dot(X_batch.T, (A - Y_batch))  # Gradient of W\n",
        "        db = (1 / m) * np.sum(A - Y_batch)  # Gradient of b\n",
        "\n",
        "        # 3) Update weights and bias using gradient descent\n",
        "        W -= alpha * dW\n",
        "        b -= alpha * db\n",
        "        # CODE END\n",
        "\n",
        "        # 4) Store the old loss so we can stop early if needed\n",
        "        prev_loss = loss\n",
        "\n",
        "        # 5) Compute the new loss using logistic_loss()\n",
        "        loss = logistic_loss(A, Y_batch)\n",
        "\n",
        "        # If the absolute change in loss is below our threshold,\n",
        "        # we break out of the loop and stop early\n",
        "        if abs(prev_loss - loss) < epsilon:\n",
        "            break\n",
        "\n",
        "        # Print progress for every 'print_every' iterations\n",
        "        if (i+1) % print_every == 0:\n",
        "\n",
        "            # Convert probabilities into binaries (0 or 1)\n",
        "            predictions = A\n",
        "            predictions[predictions >= 0.5] = 1\n",
        "            predictions[predictions < 0.5] = 0\n",
        "\n",
        "            # Compute accuracy for batch\n",
        "            accuracy = np.mean(predictions == Y_batch)\n",
        "            print(\"Iteration {}/{}: Batch Accuracy: {},  Batch Loss = {}\".format(\n",
        "                i + 1,\n",
        "                num_iterations,\n",
        "                accuracy,\n",
        "                loss\n",
        "            ))\n",
        "    # 6) Return the final learned parameters: weight vector W and bias b\n",
        "    return W, b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqT-csH_7fRb"
      },
      "source": [
        "<a id=\"logistic_regression_classifier\"></a>\n",
        "### Part 2.4 Logistic Regression Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJLCAurI7fRc"
      },
      "source": [
        "Without any further ado, here is the skeleton code for the full\n",
        "`Logistic Regression` classifier.\n",
        "Your task is to finish it up.\n",
        "You will use the functions you have implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "tags": [
          "todo"
        ],
        "id": "Ouf5ajln7fRc"
      },
      "outputs": [],
      "source": [
        "class LogisticRegressionClassifier(Classifier):\n",
        "    \"\"\"\n",
        "    TODO: Implement the Logistic Regression classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 filter_stop_words: bool = None,\n",
        "                 batch_size: int = 2000,\n",
        "                 alpha: float = 0.5,\n",
        "                 num_iterations: int = 1000,\n",
        "                 print_every: int = 100,\n",
        "                 epsilon: float = 1e-8):\n",
        "        super().__init__(filter_stop_words)\n",
        "        ngram = 1\n",
        "        tokenizer = str.split if self.filter_stop_words else None\n",
        "\n",
        "        \"\"\"\n",
        "        self.vectorizer is a countVectorizer we have created for you. Use\n",
        "        it to obtain a feature vector of word counts for each example\n",
        "        in your training data.\n",
        "        Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\n",
        "        Note: You do not need to write code to filter stop words as this is handled automatically for you\n",
        "        via the filter_stop_words param\n",
        "        \"\"\"\n",
        "        self.vectorizer = CountVectorizer(min_df=20,\n",
        "                                          ngram_range=(ngram, ngram),\n",
        "                                          stop_words=self.stop_words,\n",
        "                                          tokenizer=tokenizer)\n",
        "\n",
        "        # Parameters to use for gradient_descent()\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.num_iterations = num_iterations\n",
        "        self.print_every = print_every\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # TODO: Add other data structures needed in classify() or train()\n",
        "        # CODE START\n",
        "\n",
        "        # Initialize weights and bias\n",
        "        self.W = None\n",
        "        self.b = 0\n",
        "\n",
        "        pass\n",
        "        # CODE END\n",
        "\n",
        "    def train(self, examples: List[Example]) -> None:\n",
        "        \"\"\"\n",
        "        TODO: Implement a function to train a logistic regression model.\n",
        "\n",
        "        Args:\n",
        "            examples: A Python list containing instances of the Example class,\n",
        "                which correspond to the datapoints in the dataset.\n",
        "\n",
        "        HINTS:\n",
        "        * Use the vectorizer we have defined (self.vectorizer) to convert each\n",
        "            example into a feature vector of word counts. Read the documentation\n",
        "            for the CountVectorizer. Is there a method you can use to do this?\n",
        "        * Call gradient_descent() we have defined above to return the learned\n",
        "            weight vector and bias. You can save these for later use in\n",
        "            classify().\n",
        "        * You should use the parameters (batch_size, alpha, num_iterations,\n",
        "            print_every, epsilon) provided as arguments to the\n",
        "            LogisticRegressionClassifier when calling gradient_descent().\n",
        "        * Call self.X.toarray() after you have populated self.X with counts.\n",
        "            This converts it from a sparse matrix to a dense matrix so\n",
        "            we can use it to perform gradient descent.\n",
        "        * Depending on your implementation, your model may take some time to\n",
        "            train!\n",
        "        \"\"\"\n",
        "        # CODE START\n",
        "        texts = [example.text for example in examples]\n",
        "        labels = np.array([example.label for example in examples])\n",
        "\n",
        "        # Convert text data into feature vectors\n",
        "        self.X = self.vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "        # Train using gradient descent\n",
        "        self.W, self.b = gradient_descent(self.X, labels,\n",
        "                                          batch_size=self.batch_size,\n",
        "                                          alpha=self.alpha,\n",
        "                                          num_iterations=self.num_iterations,\n",
        "                                          print_every=self.print_every,\n",
        "                                          epsilon=self.epsilon)\n",
        "        # CODE END\n",
        "\n",
        "    def classify(self, examples: List[Example],\n",
        "                 return_scores: bool = False) -> Union[List[int], List[float]]:\n",
        "        \"\"\"\n",
        "        TODO: Implement a function to classify the given examples!\n",
        "\n",
        "        Args:\n",
        "            examples: A Python list containing instances of the Example class,\n",
        "                which correspond to the datapoints in the dataset.\n",
        "            return_scores: A boolean flag indicating whether the raw scores should\n",
        "                be returned\n",
        "\n",
        "        Returns:\n",
        "            Union[List[int], List[float]]: The classification result for each\n",
        "                example in a list. If the return_scores flag is False, the\n",
        "                function should return an object of type List[int], a list of 0\n",
        "                or 1, corresponding to each class.\n",
        "                If the return_scores flag is True, the function should instead\n",
        "                return an object of type List[float], the raw scores from the\n",
        "                sigmoid function.\n",
        "\n",
        "        HINTS:\n",
        "        * We use the `Union` class in the return type to indicate that it could\n",
        "            be a List[int] or List[float], which indicate a list containing\n",
        "            integers or a list containings floats, respectively. Whether you\n",
        "            will return a list of int or float depends on the value of the\n",
        "            return_scores flag.\n",
        "        * If sigmoid(X * W + b) is greater or equal to 0.5, the example\n",
        "            belongs to class 1, which is the positive class. Otherwise, it\n",
        "            belongs to class 0, which is the negative class. You can use the\n",
        "            sigmoid function you implemented above.\n",
        "        * You should use the weight vector and bias you computed earlier in\n",
        "            train().\n",
        "        * You can use np.dot or np.matmul to do matrix multiplication.\n",
        "        \"\"\"\n",
        "        # CODE START\n",
        "        texts = [example.text for example in examples]\n",
        "        X_test = self.vectorizer.transform(texts).toarray()\n",
        "\n",
        "        # Compute scores using the learned weights\n",
        "        scores = sigmoid(np.dot(X_test, self.W) + self.b)\n",
        "\n",
        "        if return_scores:\n",
        "            return scores.tolist()\n",
        "\n",
        "        # Convert scores to binary predictions\n",
        "        return [1 if score >= 0.5 else 0 for score in scores]\n",
        "        # CODE END\n",
        "\n",
        "    def get_weights(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        TODO: Implement a function to return the trained weights.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Trained weights, a NumPy array in the sahpe\n",
        "                (num_features,).\n",
        "        \"\"\"\n",
        "        # CODE START\n",
        "        return self.W\n",
        "        # CODE END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-ZY04F8zuVb"
      },
      "source": [
        "<a id=\"evaluation_triage\"></a>\n",
        "## Part 3. Evaluation on the Triage Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uizs3Cy0AN0"
      },
      "source": [
        "### Part 3.1. Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYgmZfrMzXzS"
      },
      "source": [
        "Once your implementation is ready, you can try evaluating it on the disaster aid classification dataset as shown below.\n",
        "Our implementation achieves the following statistics when run for the default number of iterations, so if you are getting similar results that probably means that your implementation is working well!\n",
        "\n",
        "```\n",
        "Performance on Unigrams, no stopword removal:\n",
        "Accuracy (train): 0.7903164496816497\n",
        "Accuracy (dev): 0.7726389428682472\n",
        "Performance on Unigrams with stopword removal:\n",
        "Accuracy (train): 0.7758243846811745\n",
        "Accuracy (dev): 0.7602020987174505\n",
        "```\n",
        "Our `autograder` will test the accuracy achieved by your implementation of the `LogisticRegressionClassifier` on `train`, `dev`, and `test` datasets, both with and without stop words.\n",
        "If you aren't getting the performance you are expecting in the below cell, go back to your `train` and `classify`, and `gradient_descent` methods. For example, if your loss is not going down, double check your implementation of the gradient descent function. **Remember, the classifier automatically filters stop words via the `filter_stop_words` param and the given vectorizer, so no additional code is required for stop word handling.**  \n",
        "If you are curious about the exact cutoffs we use to grade your work, you can submit your notebook to the autograder set up on `Gradescope`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "_yrkKIi2EKVy",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [],
      "source": [
        "# Load our dataset\n",
        "dataset = load_data(\"./data/triage\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [example.words for example in dataset.train]  # Use the correct attribute name\n",
        "labels = np.array([example.label for example in dataset.train])\n",
        "\n",
        "# Inspect dataset structure (optional for debugging)\n",
        "print(\"Sample instance from dataset.train:\", vars(dataset.train[0]))\n",
        "print(\"Dataset structure:\", dir(dataset))\n",
        "print(\"Dataset train size:\", len(dataset.train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW7j6TssAW5T",
        "outputId": "c9fbc776-83c1-4fd9-907f-4132845da996"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample instance from dataset.train: {'words': ['for', 'the', 'duration', 'of', 'the', 'emergency', 'operations', 'programme', 'the', 'united', 'nations', 'resident', \"coordinator's\", 'office', 'unrco', 'with', 'assistance', 'from', 'the', 'office', 'for', 'the', 'coordination', 'of', 'humanitarian', 'affairs', 'set', 'up', 'an', 'emergency', 'coordination', 'unit', 'composed', 'of', 'an', 'emergency', 'coordination', 'adviser', 'a', 'data', 'information', 'management', 'officer', 'a', 'communications', 'officer', 'and', 'a', 'media', 'and', 'public', 'relations', 'officer'], 'label': 0}\n",
            "Dataset structure: ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'dev', 'name', 'shuffle', 'splits', 'train']\n",
            "Dataset train size: 21046\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "r4rzyTrkzVO9",
        "outputId": "3cd2c816-6f31-48a1-cdb4-5180738d7c0f",
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance on Unigrams, no stopword removal:\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Example' object has no attribute 'text'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-90-2de53da05e44>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Performance on Unigrams, no stopword removal:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlr_classifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegressionClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter_stop_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Evaluate classifier with stopword removal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/util.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(classifier, dataset, limit_training_set)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlimit_training_set\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-b0555426c084>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# CODE START\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-b0555426c084>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \"\"\"\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# CODE START\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Example' object has no attribute 'text'"
          ]
        }
      ],
      "source": [
        "# Evaluate classifier without stopword removal\n",
        "print(\"Performance on Unigrams, no stopword removal:\")\n",
        "lr_classifier = LogisticRegressionClassifier(filter_stop_words=False)\n",
        "evaluate(lr_classifier, dataset)\n",
        "\n",
        "# Evaluate classifier with stopword removal\n",
        "print(\"Performance on Unigrams w/ stopword removal:\")\n",
        "lr_classifier_swr = LogisticRegressionClassifier(filter_stop_words=True)\n",
        "evaluate(lr_classifier_swr, dataset)\n",
        "\n",
        "# Evaluate classifier with stopword removal and increased iterations\n",
        "print(\"Performance on Unigrams w/ stopword removal (3000 iterations):\")\n",
        "lr_classifier_swr = LogisticRegressionClassifier(filter_stop_words=True, num_iterations=3000)\n",
        "evaluate(lr_classifier_swr, dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNcYa1XGKIIG"
      },
      "source": [
        "### Part 3.2 Sanity Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM2qYDkYKMtx"
      },
      "source": [
        "Once we've implemented and trained our model, it's often helpful to do some investigating to confirm that it's behaving the way we expect. For a `Logistic Regression` model, there are couple of different ways we can do this.\n",
        "\n",
        "If we think back to our `Naive Bayes` model, after training we have access to\n",
        "the conditional probabilities for each word (`n-gram`) given each label.\n",
        "We were able to examine them to get an idea of what words our model associates with each label.\n",
        "For our  `Logistic Regression` model, after training we have access to a weight\n",
        "vector that contains a weight for each feature.\n",
        "We can connect these weights back to the list of features (in our case, these will be unigrams).\n",
        "\n",
        "The next two cells should print your weight vectors. If the weights are zero, or if the weights are unusually large, double check your `gradient_descent` function.\n",
        "\n",
        "Features with larger weights are those that the model associates with the\n",
        "`positive` label, and those with smaller weights are those the model associates\n",
        "with the `negative` label.\n",
        "If it is not clear why this is true, try thinking back to the equation used\n",
        "to compute the `Logistic Regression` output.\n",
        "If a weight for a feature is a large positive number, and that\n",
        "feature appears in an example, what will happen to the output for\n",
        "that example?\n",
        "What about a feature with a large negative weight?\n",
        "Let's examine the features with the largest and smallest weights in our\n",
        "trained classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "tags": [
          "exploration"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "FYYxIZDh7fRj",
        "outputId": "e8a97260-3848-469d-d152-0fc9bd06e154"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CountVectorizer' object has no attribute 'get_feature_names'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-360eb7761b5e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m features_to_weights = [(features[i], weights[i])\n\u001b[1;32m      5\u001b[0m                     for i in range(len(features))]\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ],
      "source": [
        "features = lr_classifier.vectorizer.get_feature_names()\n",
        "weights = lr_classifier.get_weights()\n",
        "\n",
        "features_to_weights = [(features[i], weights[i])\n",
        "                    for i in range(len(features))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "tags": [
          "exploration"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "9AT3nwF27fRl",
        "outputId": "03e5a907-a815-4a2e-e68e-41227b6d0418"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features_to_weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-d0664b0c96a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m top_10_features = sorted(features_to_weights,\n\u001b[0m\u001b[1;32m      2\u001b[0m                    key=operator.itemgetter(1), reverse=True)[:10]\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_10_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: weight = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features_to_weights' is not defined"
          ]
        }
      ],
      "source": [
        "top_10_features = sorted(features_to_weights,\n",
        "                   key=operator.itemgetter(1), reverse=True)[:10]\n",
        "for feature, weight in top_10_features:\n",
        "    print(\"{}: weight = {}\".format(feature, weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "scrolled": true,
        "tags": [
          "exploration"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "8U-ja7BZ7fRl",
        "outputId": "7fe37153-d6f4-44c6-b42b-abf5469f6f70"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'features_to_weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-900ce0c7dc09>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bottom_10_features = sorted(features_to_weights,\n\u001b[0m\u001b[1;32m      2\u001b[0m                             key=operator.itemgetter(1))[:10]\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbottom_10_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}: weight = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'features_to_weights' is not defined"
          ]
        }
      ],
      "source": [
        "bottom_10_features = sorted(features_to_weights,\n",
        "                            key=operator.itemgetter(1))[:10]\n",
        "for feature, weight in bottom_10_features:\n",
        "    print(\"{}: weight = {}\".format(feature, weight))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc3PIN6N7fRm"
      },
      "source": [
        "**Reflection Question.**\n",
        "Do these features agree with your intuition?\n",
        "Return your answer in the `reflection_response` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "yQcAHyR17fRm"
      },
      "outputs": [],
      "source": [
        "# TODO: Include your reflection below\n",
        "def reflection_response():\n",
        "    reflection = \"Reflection\"\n",
        "    return reflection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ve3QXPqc5TGr",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "**False Positives and Negatives.** Another good thing to check is where your model made errors. In this case, our\n",
        "task was binary classification, so there are two possible types of errors\n",
        "that could have occurred:\n",
        "\n",
        "* `False Positives`: Our model predicted a high probability of label = 1 for a negative example.\n",
        "* `False Negatives`: Our model predicted a low probability of label = 1 for a positive example.\n",
        "\n",
        "We can look for exactly these two types of errors using the `return_scores`\n",
        "flag we asked you to implement for the `classify()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "j95TGjLK5TGs",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [
          "exploration"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "e550f8ef-5b2f-4466-b60e-0cfad557d2d1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFittedError",
          "evalue": "Vocabulary not fitted or provided",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-4a5a1eba8e39>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfalse_negatives\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_positives\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_false_negatives_and_false_positives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_classifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-4a5a1eba8e39>\u001b[0m in \u001b[0;36mget_false_negatives_and_false_positives\u001b[0;34m(classifier, examples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_false_negatives_and_false_positives\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpredicted_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_scores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfalse_negatives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfalse_positives\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-8ccea4f32403>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, examples, return_scores)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# CODE START\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# Compute scores using the learned weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, raw_documents)\u001b[0m\n\u001b[1;32m   1416\u001b[0m                 \u001b[0;34m\"Iterable over raw text documents expected, string object received.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             )\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m         \u001b[0;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
          ]
        }
      ],
      "source": [
        "def get_false_negatives_and_false_positives(classifier, examples):\n",
        "    predicted_scores = classifier.classify(examples, return_scores=True)\n",
        "\n",
        "    false_negatives = []\n",
        "    false_positives = []\n",
        "\n",
        "    for pred_score, example in zip(\n",
        "            predicted_scores, examples):\n",
        "        if example.label == 1 and pred_score < 0.5:\n",
        "            false_negatives.append((example.words, pred_score))\n",
        "        elif example.label == 0 and pred_score >= 0.5:\n",
        "            false_positives.append((example.words, pred_score))\n",
        "\n",
        "    return false_negatives, false_positives\n",
        "\n",
        "fn, fp = get_false_negatives_and_false_positives(lr_classifier, dataset.dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HQabfgj1Iez"
      },
      "source": [
        "Now that we have the false negatives and false positives, we can find the\n",
        "\"worst\" ones and examine them to try to figure out where our model went wrong.\n",
        "\n",
        "The **worst** ones would be:\n",
        "* The `false negatives` with the lowest probabilities of label = 1\n",
        "* The `false positives` with the highest probabilities of label = 1\n",
        "\n",
        "Run the cells below, and think about the following questions:\n",
        "* Do these errors seem reasonable?\n",
        "  Can you think of why the model may have made them?\n",
        "  Are they similar to or different from the errors you saw from the `Naive Bayes` model?\n",
        "* Did `Logistic Regression` outperform `Naive Bayes`?\n",
        "  Did you expect it to?\n",
        "  Why or why not?\n",
        "* How do the different settings (stop word removal vs. no stop word removal) affect the\n",
        "  performance relative to each other?\n",
        "  Relative to `Naive Bayes`?\n",
        "  Do these results seem reasonable?\n",
        "  If not, what might explain what you're seeing?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ogpOEF4c5TGs",
        "outputId": "0f49e23f-b0a1-4621-ef22-d745c96a3897",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-13e801e59bc7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m top_10_fn = sorted(fn,\n\u001b[0m\u001b[1;32m      2\u001b[0m                    key=operator.itemgetter(1))[:10]\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_10_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"prob = {}: {}...\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fn' is not defined"
          ]
        }
      ],
      "source": [
        "top_10_fn = sorted(fn,\n",
        "                   key=operator.itemgetter(1))[:10]\n",
        "for words, prob in top_10_fn:\n",
        "    print(\"prob = {}: {}...\".format(prob, words[:min(len(words), 10)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "4weD966k5TGt",
        "outputId": "44c7a4b7-12fa-416c-efd1-983da8feee65",
        "pycharm": {
          "name": "#%%\n"
        },
        "tags": [
          "exploration"
        ]
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'fp' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-5a0d9a69de6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No false positives found!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtop_10_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_10_fp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'fp' is not defined"
          ]
        }
      ],
      "source": [
        "if len(fp) == 0:\n",
        "    print(\"No false positives found!\")\n",
        "\n",
        "top_10_fp = sorted(fp, key=operator.itemgetter(1), reverse=True)[:10]\n",
        "for words, prob in top_10_fp:\n",
        "    print(\"prob = {}: {}\".format(prob, words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKRIDRHH5TGt",
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": [
        "Hopefully these questions have gotten you thinking about what your model\n",
        "is doing and what its weaknesses and problems might be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXl_PhMf7fRp"
      },
      "source": [
        "<a id=\"hyperparameters\"></a>\n",
        "## Part 4. Note on Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL2YPEZk7fRp"
      },
      "source": [
        "You can try adjusting the values of some of the arguments to `LogisticRegressionClassifier`, like `alpha` or `num_iterations`, re-running the evaluation, and seeing if the performance has changed.\n",
        "We haven't really discussed what these values represent, but you should be able to notice that by adjusting them you can significantly change your model's performance!\n",
        "\n",
        "These are examples of `hyperparameters` of our model.\n",
        "You can think of them as adjustable settings that control how our model works and how it learns.\n",
        "Oftentimes you will want to experiment with different choices for these parameters to find ones that work best and give the best possible performance.\n",
        "This optimal choice of the hyperparameters will depend on the particular dataset/task as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O2n-GyiIIVT"
      },
      "source": [
        "## Ending Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pI3K3ewfb2g"
      },
      "source": [
        "Congratulations, you are done with the assignment!\n",
        "Refer to the [`Submitting`](#submitting) for submission instructions."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}